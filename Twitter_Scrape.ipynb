{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import snscrape.modules.twitter as sntwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a functions that deals with user filtering and user addition\n",
    "def users_handler(user_info: sntwt.Tweet, filters: dict):\n",
    "    \"\"\"\n",
    "    this function is used to filter the given Tweet instance by performing the following processes:\n",
    "        1- extracting the user bio/description.\n",
    "        2- check if the bio contains at least one of the keywordws present in filters dict.\n",
    "        3- check if the user follwers count are greater than the limit present in filters dict.\n",
    "    if the user passes these filters his/her information will be added to a data frame.\n",
    "    \n",
    "    :param: user_info -- an instance of snscrape.modules.twitter.Tweet class contains the information about collected tweet.\n",
    "    :param: filters -- a dictionary that contains the filters which the user will be filtered against.\n",
    "    \n",
    "    :return: instance of snscrape.modules.twitter.Tweet class contains the information about the passed tweet.\n",
    "        \n",
    "    \"\"\"\n",
    "    # extracting user info from the collected tweet\n",
    "    user_bio = user_info.user.rawDescription.lower()\n",
    "    user_follower_count = user_info.user.followersCount\n",
    "    \n",
    "    if any(word.lower() in user_bio.split() for word in filters['keywords']):\n",
    "        if not any(undesired_word.lower() in user_bio.split() for undesired_word in filters['unwanted keywords']):\n",
    "            if user_follower_count > filters['followers_count']:\n",
    "                return user_info\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1            \n",
    "    else:\n",
    "        return -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def users_adder(main_user_dict, per_itr_user_dict:dict, user_info: sntwt.Tweet):\n",
    "    \"\"\"\n",
    "    this function is used to add the information of the passed user to the df\n",
    "    :param: main_user_dict -- this is the main dictionary that contains the information about the passed users\n",
    "                                {user_name:[list of usernames per user], 'url':[list of urls per user], \n",
    "                                location:[list of locations per user], #followers:[list of #followers per user]} \n",
    "    :param: per_itr_user_dict -- same as main_user_dict but it gets updated every iteration on the key word combination     \n",
    "    :param: user_info -- instance of snscrape.modules.twitter.Tweet class contains the information about the passed tweet.\n",
    "    \n",
    "    :return: tuple of dictionary that contains the passed user info and integer that represent how many user are collected.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_name = user_info.user.username\n",
    "    \n",
    "    user_bio = user_info.user.rawDescription.lower()\n",
    "    \n",
    "    user_url = user_info.url.split('status')[0]\n",
    "    \n",
    "    user_location = user_info.user.location.lower() \n",
    "    \n",
    "    try:\n",
    "        user_website = user_info.user_website\n",
    "    except AttributeError:\n",
    "        user_website = None\n",
    "    \n",
    "    user_follower_count = user_info.user.followersCount\n",
    "    \n",
    "    if not (user_name in main_user_dict['Username']): # cehck to not include duplicate data\n",
    "        \n",
    "        per_itr_user_dict['Username'].append(user_name)\n",
    "        \n",
    "        per_itr_user_dict['Bio'].append(user_bio)\n",
    "        \n",
    "        per_itr_user_dict['profile URL'].append(user_url)\n",
    "        \n",
    "        per_itr_user_dict['Location'].append(user_location)\n",
    "        \n",
    "        per_itr_user_dict['Websites'].append(user_website)\n",
    "            \n",
    "        per_itr_user_dict['#followers'].append(user_follower_count)\n",
    "    \n",
    "   \n",
    "    return per_itr_user_dict, len(per_itr_user_dict['Username'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d7a04",
   "metadata": {},
   "source": [
    "# Using Snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb7dad",
   "metadata": {},
   "source": [
    "### Algorithm description:  \n",
    "\n",
    "the general idea to find the user of interest is: to search Twitter for tweets that contain a set of keywords\n",
    "because the users which we are interested in will likely contribute to these tweets.\n",
    "we will take the following steps:\n",
    "1. create a dictionary that contains:\n",
    "    - words that we want our user's bio to include\n",
    "    - min number of followers of each user\n",
    "    - words that we do not want to include in our search\n",
    "2. generate a combination of 2 words from the previously created keywords\n",
    "3. initialize a main dictionary its key represents the required info to e collected about the users\n",
    "4. looping on the created combination and for each combination:\n",
    "    - create the search query\n",
    "    - initialize a dictionary with the same structure as the main dictionary. its purpose is to store user information\n",
    "    per iteration on the combination.\n",
    "    - for each collected tweet:\n",
    "        - check if the author of this tweet passes the specified criteria by utilizing users_handler() \n",
    "        - if the user pass, add the collected info to `per_iter_user_dict`\n",
    "        - if # the collected users are greater than 10 per combination append the collected info to the main dictionary\n",
    "        - break from the loop\n",
    "5. create a data frame from the generated dictionary and save the file as CSV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the filters up \n",
    "filters = {'keywords':['CEO', 'vice president', 'president',\n",
    "                      'chief', 'founder', 'co funder', 'CTO', 'Congress Women', 'Congress men',\n",
    "                      'senator', 'MP', 'parliament', 'head', 'senior', 'Activist', 'creator', 'board member',\n",
    "                      'Chairman', 'VP', 'Boss'],\n",
    "           'unwanted keywords': ['sex', 'porn', 'adult', 'PLAYMATE', 'Model'],\n",
    "           'followers_count':10000,}\n",
    "\n",
    "# generating combination of the desired words 2 at a time\n",
    "desired_words_combinations = list(itertools.combinations(filters['keywords'], 2))\n",
    "\n",
    "# setting the unwanted words in tweets\n",
    "undsired_words = ' -'.join(filters['unwanted keywords'])\n",
    "\n",
    "main_user_dict = {'Username':[], 'Bio':[], 'profile URL':[], 'Location':[], 'Websites':[],'#followers':[]}\n",
    "\n",
    "for word in desired_words_combinations:\n",
    "    # setting the query\n",
    "    desired_words = ' OR '.join(list(word))\n",
    "    query = '({}) -{} lang:en until:2023-01-07 since:2020-01-01'.format(desired_words, undsired_words)\n",
    "    print(\"search query is: {} \\n\".format(query))\n",
    "    \n",
    "    per_iter_user_dict = {'Username':[], 'Bio':[], 'profile URL':[], 'Location':[], 'Websites':[],'#followers':[]}\n",
    "    \n",
    "    for i, tweet in enumerate(sntwt.TwitterSearchScraper(query).get_items()):\n",
    "\n",
    "        responce = users_handler(tweet, filters)\n",
    "        if responce != -1 :\n",
    "            user_dict, collected_users = users_adder(main_user_dict, per_iter_user_dict, tweet)\n",
    "            if collected_users > 10:   \n",
    "                print('breaking the loop')\n",
    "                main_user_dict['Username'].extend(per_iter_user_dict['Username'])\n",
    "                main_user_dict['Bio'].extend(per_iter_user_dict['Bio'])\n",
    "                main_user_dict['profile URL'].extend(per_iter_user_dict['profile URL'])\n",
    "                main_user_dict['Location'].extend(per_iter_user_dict['Location'])\n",
    "                main_user_dict['Websites'].extend(per_iter_user_dict['Websites'])\n",
    "                main_user_dict['#followers'].extend(per_iter_user_dict['#followers'])\n",
    "                \n",
    "                break\n",
    "            elif i % 100 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            elif collected_users % 2 == 0:\n",
    "                print(\"\\n collected {} users\".format(collected_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aad8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(main_user_dict)\n",
    "final_df = df.drop(['#followers'], axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ae390",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd481d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"Twitter_user_data\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Twitter_user_data_#count\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59018d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec97fd",
   "metadata": {},
   "source": [
    "# Using Twitter API and Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a functions that deals with user filtering and user addition\n",
    "def users_handler(user_info: tweepy.models.User, filters: dict):\n",
    "    \"\"\"\n",
    "    this function is used to filter the given Tweet instance by performing the following processes:\n",
    "        1- extracting the user bio/description.\n",
    "        2- check if the bio contains at least one of the keywordws present in filters dict.\n",
    "        3- check if the user follwers count are greater than the limit present in filters dict.\n",
    "    if the user passes these filters his/her information will be added to a data frame.\n",
    "    \n",
    "    :param: user_info -- an instance of snscrape.modules.twitter.Tweet class contains the information about collected tweet.\n",
    "    :param: filters -- a dictionary that contains the filters which the user will be filtered against.\n",
    "    \n",
    "    :return: instance of snscrape.modules.twitter.Tweet class contains the information about the passed tweet.\n",
    "        \n",
    "    \"\"\"\n",
    "    # extracting user info from the collected tweet\n",
    "    user_bio = user_info.description.lower()\n",
    "    user_follower_count = user_info.followers_count\n",
    "    \n",
    "    if any(word.lower() in user_bio.split() for word in filters['keywords']):\n",
    "        if not any(undesired_word.lower() in user_bio.split() for undesired_word in filters['unwanted keywords']):\n",
    "            if user_follower_count > filters['followers_count']:\n",
    "                return user_info\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1            \n",
    "    else:\n",
    "        return -1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def users_adder(main_user_dict:dict, per_itr_user_dict:dict, user_info: tweepy.models.User):\n",
    "    \"\"\"\n",
    "    this function is used to add the information of the passed user to the df\n",
    "    :param: main_user_dict -- this is the main dictionary that contains the information about the passed users\n",
    "                                {user_name:[list of usernames per user], 'url':[list of urls per user], \n",
    "                                location:[list of locations per user], #followers:[list of #followers per user]} \n",
    "    :param: per_itr_user_dict -- same as main_user_dict but it gets updated every iteration on the key word combination     \n",
    "    :param: user_info -- instance of snscrape.modules.twitter.Tweet class contains the information about the passed tweet.\n",
    "    \n",
    "    :return: tuple of dictionary that contains the passed user info and integer that represent how many user are collected.\n",
    "    \"\"\"\n",
    "\n",
    "    user_name = user_info.screen_name\n",
    "    \n",
    "    user_bio = user_info.description.lower()\n",
    "    \n",
    "    user_url = \"https://twitter.com/{}\".format(tweet.screen_name)\n",
    "    \n",
    "    user_location = user_info.location.lower() \n",
    "    \n",
    "    user_website = user_info.url\n",
    "    \n",
    "    user_follower_count = user_info.followers_count\n",
    "    \n",
    "    if not (user_name in main_user_dict['Username']): # cehck to not include duplicate data, index=False\n",
    "        \n",
    "        per_itr_user_dict['Username'].append(user_name)\n",
    "        \n",
    "        per_itr_user_dict['Bio'].append(user_bio)\n",
    "        \n",
    "        per_itr_user_dict['profile URL'].append(user_url)\n",
    "        \n",
    "        per_itr_user_dict['Location'].append(user_location)\n",
    "        \n",
    "        per_itr_user_dict['Websites'].append(user_website)\n",
    "            \n",
    "        per_itr_user_dict['#followers'].append(user_follower_count)\n",
    "    \n",
    "   \n",
    "    return per_itr_user_dict, len(per_itr_user_dict['Username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read configs\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key = config['twitter']['api_key']\n",
    "api_key_secret = config['twitter']['api_key_secret']\n",
    "\n",
    "access_token = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']\n",
    "\n",
    "# authentication\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061afbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth ,wait_on_rate_limit=True)\n",
    "\n",
    "public_tweets = api.home_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb36f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the filters up cx\n",
    "filters = {'keywords':['CEO', 'vice president', 'president',\n",
    "                      'chief', 'founder', 'co funder', 'CTO', 'Congress Women', 'Congress men',\n",
    "                      'senator', 'MP', 'parliament', 'head', 'senior', 'Activist', 'creator', 'board member',\n",
    "                      'Chairman', 'VP', 'Boss'],\n",
    "           'unwanted keywords': ['sex', 'porn', 'adult', 'PLAYMATE', 'Model'],\n",
    "           'followers_count':10000,}\n",
    "\n",
    "# generating combination of the desired words 2 at a time\n",
    "desired_words_combinations = list(itertools.combinations(filters['keywords'], 2))\n",
    "\n",
    "# setting the unwanted words in tweets\n",
    "undsired_words = ' -'.join(filters['unwanted keywords'])\n",
    "\n",
    "main_user_dict = {'Username':[], 'Bio':[], 'profile URL':[], 'Location':[], 'Websites':[],'#followers':[]}\n",
    "\n",
    "for word in desired_words_combinations:\n",
    "    # setting the query\n",
    "    desired_words = ' OR '.join(list(word))\n",
    "    query = '({}) -{} lang:en'.format(desired_words, undsired_words)\n",
    "    print(\"search query is: {} \\n\".format(query))\n",
    "    \n",
    "    per_iter_user_dict = {'Username':[], 'Bio':[], 'profile URL':[], 'Location':[], 'Websites':[],'#followers':[]}\n",
    "    \n",
    "    tweets = tweepy.Cursor(api.search_users, q=query , count=20, include_entities=True).items(500)\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        responce = users_handler(tweet, filters)\n",
    "        if responce != -1 :\n",
    "            user_dict, collected_users = users_adder(main_user_dict, per_iter_user_dict, tweet)\n",
    "            if collected_users > 3:   \n",
    "                main_user_dict['Username'].extend(per_iter_user_dict['Username'])\n",
    "                main_user_dict['Bio'].extend(per_iter_user_dict['Bio'])\n",
    "                main_user_dict['profile URL'].extend(per_iter_user_dict['profile URL'])\n",
    "                main_user_dict['Location'].extend(per_iter_user_dict['Location'])\n",
    "                main_user_dict['Websites'].extend(per_iter_user_dict['Websites'])\n",
    "                main_user_dict['#followers'].extend(per_iter_user_dict['#followers'])\n",
    "                print('breaking the loop')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0016cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(main_user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272115a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c87113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['#followers'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb29eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Twitter_Users_Data1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e871e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Omdena",
   "language": "python",
   "name": "omdena"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
